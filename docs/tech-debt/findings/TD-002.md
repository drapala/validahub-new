# TD-002 — Sistema de Job Queue em Memória

**Dimensão:** Arquitetura  
**Arquivo(s)/Local:** 
- `apps/api/src/api/v1/jobs.py:12-13`
- `apps/api/src/api/v1/validation.py:654, 667`

**Evidência:**

```python
# jobs.py:12-13
# TODO: Replace with Redis or database
job_storage: Dict[str, Any] = {}

# validation.py:654
# TODO: Implement with Celery or similar job queue
async def process_validation_async(job_id: str, ...):
    
# validation.py:667  
# TODO: Implement with Celery or similar job queue
```

**Por que importa:** 
- **Perda de dados**: Jobs são perdidos em restart/crash
- **Não escalável**: Limitado a uma única instância
- **Sem retry**: Falhas não são recuperadas
- **Sem priorização**: FIFO simples sem controle
- **Memory leak**: Jobs acumulam em memória

**Opções de correção:**

1. **Celery + Redis** (Recomendada)
   - Prós: Maduro, bem documentado, retry automático
   - Contras: Complexidade adicional, worker separado
   
2. **FastAPI Background Tasks + DB**
   - Prós: Simples, sem deps extras
   - Contras: Limitado, sem features avançadas

3. **RQ (Redis Queue)**
   - Prós: Mais simples que Celery
   - Contras: Menos features, menos adoção

**Proposta:**

```diff
--- a/apps/api/requirements.txt
+++ b/apps/api/requirements.txt
@@ -15,3 +15,5 @@
 pandas==2.0.3
 openpyxl==3.1.2
 chardet==5.2.0
+celery==5.3.4
+redis==5.0.1

--- /dev/null
+++ b/apps/api/src/workers/celery_app.py
@@ -0,0 +1,35 @@
+from celery import Celery
+import os
+
+celery_app = Celery(
+    "validahub",
+    broker=os.getenv("REDIS_URL", "redis://localhost:6379/0"),
+    backend=os.getenv("REDIS_URL", "redis://localhost:6379/0"),
+    include=["src.workers.tasks"]
+)
+
+celery_app.conf.update(
+    task_serializer="json",
+    accept_content=["json"],
+    result_serializer="json",
+    timezone="UTC",
+    enable_utc=True,
+    task_track_started=True,
+    task_time_limit=30 * 60,  # 30 min
+    task_soft_time_limit=25 * 60,  # 25 min
+    task_acks_late=True,
+    worker_prefetch_multiplier=1,
+    task_retry_max=3,
+    task_retry_backoff=True,
+    task_retry_backoff_max=600,
+    task_retry_jitter=True,
+)
+
+--- /dev/null  
+++ b/apps/api/src/workers/tasks.py
@@ -0,0 +1,87 @@
+from celery import Task
+from .celery_app import celery_app
+import pandas as pd
+from typing import Dict, Any
+import logging
+
+logger = logging.getLogger(__name__)
+
+class CallbackTask(Task):
+    """Task with progress callback support."""
+    
+    def on_success(self, retval, task_id, args, kwargs):
+        """Success handler."""
+        logger.info(f"Task {task_id} succeeded")
+    
+    def on_failure(self, exc, task_id, args, kwargs, einfo):
+        """Failure handler."""
+        logger.error(f"Task {task_id} failed: {exc}")
+
+@celery_app.task(bind=True, base=CallbackTask, name="validate_csv")
+def validate_csv_task(
+    self,
+    file_path: str,
+    marketplace: str,
+    category: str,
+    options: Dict[str, Any]
+) -> Dict[str, Any]:
+    """
+    Async CSV validation task.
+    
+    Args:
+        file_path: Path to uploaded file
+        marketplace: Target marketplace
+        category: Product category
+        options: Validation options
+        
+    Returns:
+        Validation results
+    """
+    try:
+        # Update progress
+        self.update_state(
+            state="PROGRESS",
+            meta={"current": 0, "total": 100, "status": "Loading file..."}
+        )
+        
+        # Load file
+        df = pd.read_csv(file_path)
+        total_rows = len(df)
+        
+        # Import here to avoid circular dependency
+        from src.services.rule_engine_service import RuleEngineService
+        from src.core.pipeline.validation_pipeline import ValidationPipeline
+        
+        # Initialize services
+        rule_engine = RuleEngineService()
+        pipeline = ValidationPipeline(rule_engine)
+        
+        # Process with progress updates
+        results = []
+        for i, row in df.iterrows():
+            if i % 100 == 0:
+                self.update_state(
+                    state="PROGRESS", 
+                    meta={
+                        "current": i,
+                        "total": total_rows,
+                        "status": f"Processing row {i}/{total_rows}"
+                    }
+                )
+            
+            # Validate row
+            result = pipeline.validate_row(
+                row, marketplace, category, options
+            )
+            results.append(result)
+        
+        return {
+            "status": "completed",
+            "total_rows": total_rows,
+            "results": results
+        }
+        
+    except Exception as e:
+        logger.error(f"Validation task failed: {e}")
+        self.retry(exc=e)

--- a/apps/api/src/api/v1/validation.py
+++ b/apps/api/src/api/v1/validation.py
@@ -652,15 +652,26 @@ async def validate_csv_async(
         detail="File size exceeds async limit"
     )
     
-    # TODO: Implement with Celery or similar job queue
-    job_id = str(uuid.uuid4())
+    # Save file temporarily
+    import tempfile
+    with tempfile.NamedTemporaryFile(delete=False, suffix=".csv") as tmp:
+        content = await file.read()
+        tmp.write(content)
+        tmp_path = tmp.name
     
-    # For now, store in memory (not production ready)
-    job_storage[job_id] = {
-        "status": "pending",
-        "created_at": datetime.utcnow().isoformat()
-    }
+    # Submit to Celery
+    from src.workers.tasks import validate_csv_task
+    task = validate_csv_task.delay(
+        file_path=tmp_path,
+        marketplace=marketplace,
+        category=category,
+        options={
+            "auto_fix": auto_fix,
+            "validate_images": validate_images,
+            "check_duplicates": check_duplicates
+        }
+    )
     
+    job_id = task.id
     return AsyncJobResponse(
         job_id=job_id,
         status="accepted",
```

**Esforço:** L (3-5 dias)  
**Aceite:**
- Jobs persistem entre restarts
- Workers escalam horizontalmente
- Retry automático com backoff
- Progress tracking funcional
- Testes de resiliência passando

**Links:**
- [Celery Documentation](https://docs.celeryproject.org/)
- [FastAPI + Celery Integration](https://fastapi.tiangolo.com/tutorial/background-tasks/)